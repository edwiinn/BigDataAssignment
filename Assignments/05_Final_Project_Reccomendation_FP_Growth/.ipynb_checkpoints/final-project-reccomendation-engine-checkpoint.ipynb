{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Product Reccomendation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Batch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"mode\", \"DROPMALFORMED\").load(\"batch1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- city_category: string (nullable = true)\n",
      " |-- stay_in_current_city_years: string (nullable = true)\n",
      " |-- marital_status: string (nullable = true)\n",
      " |-- product_category_1: string (nullable = true)\n",
      " |-- product_category_2: string (nullable = true)\n",
      " |-- product_category_3: string (nullable = true)\n",
      " |-- purchase: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slice Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "userProductDf = df[[\"user_id\",\"product_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ageProductDf = df[[\"age\",\"product_id\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1): \n",
    "  \n",
    "    # intilize a null list \n",
    "    unique_list = [] \n",
    "      \n",
    "    # traverse for all elements \n",
    "    for x in list1:\n",
    "        x = x.strip()\n",
    "        # check if exists in unique_list or not \n",
    "        if x not in unique_list: \n",
    "            unique_list.append(x)\n",
    "      \n",
    "    return unique_list\n",
    "def fudf(val):\n",
    "    return unique(val)\n",
    "flattenUdf = F.udf(fudf, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By UserID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|            ListItem|\n",
      "+-------+--------------------+\n",
      "|1000240|[P00106742, P0022...|\n",
      "|1000280|[P00003442, P0030...|\n",
      "|1000665|[P00116742, P0007...|\n",
      "|1000795|[P00289942, P0011...|\n",
      "|1000839|[P00184942, P0011...|\n",
      "|1000888|[P00147742, P0011...|\n",
      "|1001866|[P00154642, P0029...|\n",
      "|1002011|[P00142942, P0011...|\n",
      "|1002185|[P00284642, P0017...|\n",
      "|1002442|[P00236942, P0028...|\n",
      "|1002783|[P00070542, P0011...|\n",
      "|1002883|[P00249542, P0021...|\n",
      "|1002887|[P00157642, P0025...|\n",
      "|1003202|[P00157642, P0007...|\n",
      "|1003366|[P00351142, P0019...|\n",
      "|1003397|[P00025442, P0008...|\n",
      "|1003644|         [P00009542]|\n",
      "|1003663|[P00080342, P0012...|\n",
      "|1003665|[P00080342, P0011...|\n",
      "|1004042|[P00351342, P0034...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupedUserProductDf = userProductDf.groupBy('user_id').agg(F.collect_list('product_id').alias(\"ListItem\"))\n",
    "groupedUserProductDf = groupedUserProductDf.select(\"user_id\", flattenUdf(\"ListItem\").alias(\"ListItem\"))\n",
    "groupedUserProductDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with FPGrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpGrowth = FPGrowth(itemsCol=\"ListItem\", minSupport=0.004, minConfidence=0.05)\n",
    "model = fpGrowth.fit(groupedUserProductDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-------------------+\n",
      "| antecedent| consequent|         confidence|\n",
      "+-----------+-----------+-------------------+\n",
      "|[P00046742]|[P00112142]|0.10606060606060606|\n",
      "|[P00046742]|[P00034742]|0.09090909090909091|\n",
      "|[P00220442]|[P00112142]|0.09881422924901186|\n",
      "|[P00220442]|[P00031042]|0.09486166007905138|\n",
      "|[P00025442]|[P00112142]|0.08196721311475409|\n",
      "|[P00025442]|[P00110742]|0.09180327868852459|\n",
      "|[P00025442]|[P00057642]|0.10819672131147541|\n",
      "|[P00025442]|[P00058042]|0.08524590163934426|\n",
      "|[P00025442]|[P00110942]|0.07868852459016394|\n",
      "|[P00000142]|[P00117442]|0.12077294685990338|\n",
      "|[P00278642]|[P00184942]| 0.1111111111111111|\n",
      "|[P00278642]|[P00265242]| 0.1282051282051282|\n",
      "|[P00110942]|[P00025442]|0.09561752988047809|\n",
      "|[P00010742]|[P00145042]| 0.1016949152542373|\n",
      "|[P00028842]|[P00058042]|0.11059907834101383|\n",
      "|[P00117942]|[P00112142]|0.10588235294117647|\n",
      "|[P00145042]|[P00010742]|0.09195402298850575|\n",
      "|[P00031042]|[P00220442]|0.09795918367346938|\n",
      "|[P00031042]|[P00058042]|0.10612244897959183|\n",
      "|[P00265242]|[P00278642]|0.09900990099009901|\n",
      "+-----------+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.associationRules.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "temp = [Row(ListItem=[\"P00220442\",\"P00112142\"])]\n",
    "temp = spark.createDataFrame(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------------------------------------+\n",
      "|ListItem              |prediction                                  |\n",
      "+----------------------+--------------------------------------------+\n",
      "|[P00220442, P00112142]|[P00031042, P00025442, P00046742, P00117942]|\n",
      "+----------------------+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(temp).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
